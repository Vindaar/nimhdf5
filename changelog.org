* v0.5.3
- add basic serialization submodule to auto serialize most objects to
  a H5 file. Scalar types are written as attributes and non scalar as
  datasets.
  Can be extended for complicated custom types by using the ~toH5~
  hook. See the ~tSerialize.nim~ test and the ~serialize.nim~ file.
  Note: currently no deserialization is supported. You need to parse
  the data back into your file if needed. An equivalent inverse can be
  added, but has no priority at the moment.
* v0.5.2
- remove support for reading into a ~cstring~, as this is not well
  defined. A local cstring that needs to be created cannot be returned
  (without dealing manually with allocations)
- add ~add~, ~write_hyperslab~, ~read~ working with ~ptr T~ for direct
  access with a manual memory region (useful when working with things
  like ~Tensors~)
- reorder ~dataset.nim~ code a little bit
- support ~openArray~ in more places    
* v0.5.1
- (finally!) add support for =string= datasets
  - fixed length string datasets, written by constructing a
    =create_dataset("foo", <size>, array[N, char])= dataset (writing
    is done by simply giving a =seq[string]=
  - variable length string datasets, written by constructing a
    =create_dataset("foo", <size>, string)= dataset (writing
    is done by simply giving a =seq[string]=)
  - support strings as variable length arrays of type =char=,
    constructed by =create_dataset("foo", <size>, special_type(char))= dataset (writing
    is done by simply giving a =seq[string]=
- add missing overload for =write= for the most general case, which
  was previously only possible via ~[]=~, so:
  #+begin_src nim
  let dset = ...
  dset.write(data)
  #+end_src
  is now valid.
- implement slicing =read= and =write= procedures for 1D datasets:
  #+begin_src nim
  let data = @[1, 2, 3]
  var dset = h5f.create_dataset("foo", 3, int)
  dset.write(data)
  doAssert data[0 .. 1] == data[0 .. 1]
  doAssert data.read(0 .. 1) == data[0 .. 1]
  dset.write(1 .. 2) = @[4, 5]
  doAssert dset[1 ..< 3] == @[4, 5]
  dset[0 .. 1] = @[10, 11]
  doAssert dset[int] == @[10, 11, 5]
  #+end_src
  is now also all valid. These are implemented by using hyperslab
  reading / writing.
- fix bug in =write_norm= about coordinate selection, such that
  writing specific indices now actually works correctly
- fix bug in =write= when writing specific coordinates of a 1D dataset  
* v0.5.0
- fix behavior of =delete= to make sure we also keep our internal
  =TableRef= in line with the file
- *BREAKING:* fully support writing datasets as =(N, )= instead of turning it into =(N,
  1)= instead (especially for VLEN data).
  This has big implications when reading 1D data using hyperslabs. If
  instead of adding an extra dimension as:
  #+begin_src nim
  let data = dset.read_hyperslab(dtype, start = @[1000, 0], count = @[1000, 1])
  #+end_src
  instead of
  #+begin_src nim
  let data = dset.read_hyperslab(dtype, start = @[1000], count = @[1000])
  #+end_src
  reading performance is *orders of magnitudes* slower!
  Essentially when handing an integer to =create_datasets= it is now
  kept as such (and turned into a 1 element tuple). 
  For non vlen data creating and writing such datasets correctly
  worked correctly before if I'm not mistaken.
- add more exception types for dealing with filters & in particular
  =blosc=:
  - =HDF5FilterError=
  - =HDF5DecompressionError=
  - =HDF5BloscFilterError=
  - =HDF5BloscDecompressionError=
* v0.4.7
- add =overwrite= option to =write_dataset= convenience proc
* v0.4.6
- avoid copy of input data when writing VLEN data
- CT error if composite data with string fields is being read, as it's
  currently not supported (strings are vlen data & vlen in composite
  isn't implemented)
- fix regression in =copy= due to =distinct hid_t= variants
- extend =withDset= to work properly with vlen data (returning =dset=
  variable with =seq[seq[T]]=) and add =withDset= overload working
  with a H5 file and a string name of a dataset
- add test case for =withDset=  
* v0.4.5
- treat =akTruncate= flag as write access to the file
  (=create_dataset= was not working with it)
- fix =blosc= filter, regressed due to recent =distinct= introductions
* v0.4.4
- further fixes ~=destroy~ hooks introduced in =v0.4.2=. Under some
  circumstances the defined hooks caused segmentation faults when
  deallocating objects (these hooks are finicky!)
- fix opening files with =akTruncate= (i.e. overwrite a file instead
  of appending)
- *SEMI-BREAKING*: raise an exception if opening a file failed.
  This is more of an oversight rather than a feature that we did not
  raise so far. This is not really *breaking* in a sense, because in
  the past we simply failed in the =getNumAttrs= call that happened
  when trying to open the attributes of the root group in the file.
* v0.4.3
- fixed the ~=destroy~ hooks introduced in =v0.4.2=
- added support for =SWMR= (see README)
- introduce better checks on whether an object is open by using =H5I=
  interface
- turn file access constants into an =enum= to better handle multiple
  constants at the same time as a =set=
- lots of cleanup of old code, replace includes by imports, ...  
* v0.4.2
- adds =getOrCreateGroup= helper to always get a group, either
  returning the existing one or creating it.
  Before version =v0.4.0= this was the default behavior for =[]= as
  well as =create_group=.
  As of now, =[]= raises a =KeyError= now if it does not exist (this
  is a *breaking* change that is retroactively added to the changelog
  of =v0.4.0=). However, =create_group= does *not* throw if the group
  already exists. This may change in the future though.
* v0.4.1
- adds missing import of =os.`/`= in =datasets.nim=, which got removed
  in the refactor
- fixes a regression in =open= for datasets in the case of a not
  existing dataset
* v0.4.0
- *NOTE:* At the time of release of =v0.4.0= the following *breaking*
  change was not listed as such:
  - =[]= for groups does *not* create a group anymore, if it does not
    exist. Use =getOrCreateGroup= added in =v0.4.2= for that! This was
    an unintended side effect that was overlooked, as the
    implementation was based on =create_group=.
- *major* change: introduce multiple different distinct types for the
  different usages of =hid_t= in the HDF5 library. This gives us more
  readability, type safety etc. We can write proper type aware =close=
  procedures etc.
- also adds ~=destroy~ hooks for all relevant types, so manual closing
  is not required anymore (unless one wishes to close early)
- *breaking*: iterators taking a =depth= argument now treat it
  differently. A depth of 0 now means *only the same level* where
  previously it meant *all levels*. The previous behavior is available
  via ~depth = -1~. The default behavior has not changed though.
- *breaking*: renames the =shape_raw= and =dset_raw= arguments of =create_dataset= to
  simply =shape= and =dset=. The purpose of the =_raw= suffix is completely
  unimportant for a user of the library.
- improve output of pretty printing of datasets, groups and files
- add tests for iterators and =contains= procedure
* v0.3.16
- refactor out pretty printing, iterators, some attribute related code
  into their own files
- move constructors into =datatypes.nim=, as they don't depend on
  other things and are often useful in other modules (better
  separation, less recursive imports)
- move a lot of features into =h5util= that may be used commonly
  between modules
- fixes issue with iterator for groups, which could cause to not find
  any datasets in a group, despite them existing
* v0.3.15
- fix segmentation fault in =visit_file= for C++ backend
* v0.3.14
- fix =H5Attributes= return values for =[]= template returning
  =AnyKind=
- change =[]=, ~[]=~ templates for =H5Attribtutes= into procs
- fix the high level example to at least make it compile  
* v0.3.13
- =visit_file= now does not open all groups and datasets anymore. Only
  recognizes which groups / files actually exist
- adds =close= for dataset / groups. Both are now aware if they are
  open or not
- add a string conversion for =H5Attr=
- fix accessing a dataset from a group. Now uses the path of the group
  as the base
- fix error message in =read_hyperslab_vlen=
- turn some templates into procs
- make =blosc= an optional import
* v0.3.12
- =H5File= as a proc is deprecated and replaced by =H5open=!
- reading of string attributes now takes care to check if they are
  variable length or fixed length strings
- import of =blosc= plugin is not automatic anymore, but needs to be
  done manually by compiling with =-d:blosc= 
- remove a lot old comments and imports from days past...
* v0.3.11
- change usage of =csize= to =csize_t= in full wrapper / library. For
  most use cases this did not have any effect (=csize= was an int,
  instead of unsigned). But for =H5T_VARIABLE = csize.high= this
  caused problems, because the value was not the one expected
  (=csize_t.high=)
- add support for compound datatypes. Creating a dataset / writing and
  reading data works for any objects `T` which have fields that can be
  stored in HDF5 files currently. 
  Objects and tuples are treated the same!
- add support for =seq[string]= attributes
- reorder =datasets.nim= and clean up =[]= logic
- add =[]= accessor from a =H5Group=
- add =isVlen= helper to check if dataset is variable length
- make =special_type= usage optional when reading datasets
- fix branching in =nimToH5type= to be fully compile time
- add =H5File= to replace =H5FileObj= (latter is kept as deprecated
  typedef)
- variable lenght data is created automatically if user gives =seq[T]=
  type in =create_dataset=
- =read= can automatically read variable length data if =seq[T]=
  datatype is given
- add tests for compound data and =seq[string]= attributes
* v0.3.10
- change =dtypeAnyKind= definition when creating dataset
- improve iteration over subgroups / datasets
* v0.3.9
- fix mapping of H5 types to Nim types, see PR #36.

* v0.3.8

- remove dependency of =typetraits= and =typeinfo= modules by
  introducing custom =DtypeKind enum=
